+++
categories = ['Papers', 'RL Series']
citations = true
date = '2022-06-14'
draft = true
katex = true
slug = 'critic-regularized-regression'
subtitle = 'Tips and tricks for Offline RL'
title = 'Critic Regularized Regression'
+++
This post describes the important ideas behind Critic Regularized Regression {{<cite "wang2020critic">}}.

# 1. Introduction
---
An important factor behind the success of RL algorithms has been the ability to run and evaluate policies during training. This paradigm is called online RL. A complimentary approach is offline RL, where we don't have access to the environment during training. The agent needs to learn using samples from a fixed dataset $B$. This paradigm is particularly interesting because of the evaluation crisis in RL, which refers to the lack of proper datasets for side-by-side comparison of RL algorithms. Naive application of off-policy learning algorithms does not work. Here's why.

## Bootstrapping
Bootstrapping is the practice of updating a function based on future estimates from the same function. In the TD(0) update shown below, where we use $R_{t+1} + \gamma V(s')$ as an estimate of the true value $V(s)$. However, we're estimating a guess $V(s)$ based on another guess $V(s')$. 
$$V(s) = V(s) + \alpha(\underbrace{R_{t+1} + \gamma V(s')}_{\text{TD Target}} - V(s))$$

## Overestimation
Overestimation is when the learnt estimate is higher than the true value. In the Q learning update shown below, the maximization operator $\max_{a} Q(s', a)$ results in overestimation of Q values. We are interested in $\max_{a} \mathbb{E}[Q(s', a)]$. Because of the maximization operator, we end up estimating $\mathbb{E}[\max_{a} Q(s', a)]$ instead, which is biased. For more details, see {{<cite "hasselt2010double" "text">}}
$$Q(s, a) = Q(s, a) + \alpha(\overbrace{R_{t+1} + \gamma \max_{a} Q(s', a)}^{\text{TD Target}} - Q(s, a))$$

## Extrapolation
Extrapolation is the unrealistic estimation of Q values for state-action pairs that are not present in the dataset. As you can imagine, this problem is specific to offline RL. Unlike online RL, we can't run the current policy on unseen data to get rewards for Q updates.


# 2. Critic Regularized Regression
---
One method to do away with the problem of overestimation is to prevent querying the Q function if the sample is not in the dataset.

# References
{{<bibliography "cited">}}